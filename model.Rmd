---
title: "Statistical Model Analysis"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
library(tidyverse)
library(plotly)

wine_rating = read_csv("./wine_rating.csv")
```

## T-test to compare mean price among four categories.

To better study the average prices of the various wine categories in our dataset Pairwise Welch Two Sample t-tests were used in your analytical method. We think the Welch t-test is a reliable option for real-world since it is primarily used in situations when the variances of the two groups being compared may not be identical.

```{r}
# Filter the data for the four categories
red_wines <- subset(wine_rating, categories == "Red")
white_wines <- subset(wine_rating, categories == "White")
rose_wines <- subset(wine_rating, categories == "Rose")
sparkling_wines <- subset(wine_rating, categories == "Sparkling")

# Perform T-tests &  Print results
# Comparing Red and White
t_test_red_white <- t.test(red_wines$price, white_wines$price)
print(t_test_red_white)
```

A very significant difference in the mean prices of red and white wines is shown by the Welch Two Sample t-test, with a t-statistic of 17.771 and a p-value less than 2.2e-16. This test has 12155 degrees of freedom, which suggests a sizable sample size. The confidence interval, which spans from 16.48147 to 20.56802, indicates that red wines regularly cost much more than white wines. The average cost of red wines is around 39.15, while the average cost of white wines is about 20.62. This significant price differential between the two groups reflects their different market positioning, with red wines perhaps being seen as more desired or premium in specific situations.


### Comparing Red and Rose

```{r}
t_test_red_rose <- t.test(red_wines$price, rose_wines$price)
print(t_test_red_rose)
```

The Welch t-test results in a t-statistic of 21.878 with a p-value less than 2.2e-16 when red and rose wines are compared, suggesting an even more marked difference in mean pricing than there is between red and white wines. Here, the sample sizes and variances are reflected in the 1922.4 degrees of freedom. Red wines are much more costly than rose wines, as seen by the confidence interval, which spans from 24.23314 to 29.00550. The mean prices of red wines are 39.15, while those of rose wines are 12.53. This finding implies that there is a distinct market distinction between these groups, with red wines being priced more.

### Comparing Red and Sparkling

```{r}
t_test_red_sparkling <- t.test(red_wines$price, sparkling_wines$price)
print(t_test_red_sparkling)
```

The t-statistic for the comparison of red and sparkling wines is 2.4847, with a p-value of 0.01305, suggesting a statistically significant difference that is not as strong as it was in the earlier comparisons.  The smaller confidence range, which falls between 7.7685347 and 0.9147425, indicates a slight variation in mean prices. The mean price of red wines is roughly 39.15, which is somewhat more than the mean price of sparkling wines, which is approximately 34.80. A tighter pricing range is seen in this comparison between the two groups, maybe as a result of comparable manufacturing costs or overlapping market sectors.

### Comparing White and Rose

```{r}
t_test_white_rose <- t.test(white_wines$price, rose_wines$price)
print(t_test_white_rose)
```

A considerable price difference between white and rose wines is revealed by the t-test, which has a t-statistic of 8.5229 and a p-value of less than 2.2e-16. With a mean price of 20.62, white wines are considerably more costly than rose wines, which have a mean price of 12.53. This is evident from the confidence interval, which spans from 6.230128 to 9.959023. This discrepancy suggests that these two wine varieties have different market positionings as well as potential variations in manufacturing processes or customer perceptions.

### Comparing White and Sparkling

```{r}
t_test_white_sparkling <- t.test(white_wines$price, sparkling_wines$price)
print(t_test_white_sparkling)
```

The Welch t-test reveals a t-statistic of -9.0158 and a p-value less than 2.2e-16 when comparing the mean prices of white and sparkling wines, suggesting a significant difference. According to the negative t-statistic, sparkling wines cost more than white wines. The average cost of sparkling wines is around 34.80, which is much more than the average cost of white wines, which is 20.62. This finding emphasizes sparkling wines' premium positioning, which may be related to their connotation of luxury and festivities.

### Comparing Rose and Sparkling

```{r}
t_test_rose_sparkling <- t.test(rose_wines$price, sparkling_wines$price)
print(t_test_rose_sparkling)
```

With a t-statistic of -13.153 and a p-value smaller than 2.2e-16, the t-test comparing rose and sparkling wines reveals a very significant difference in their mean costs. Sparkling wines, with a mean price of 34.80, are significantly more costly than rose wines, with a mean price of 12.53, according to the negative t-statistic and the confidence interval spanning from -25.60013 to -18.95523. The notable disparity in price between these two wine categories might perhaps be attributed to variations in manufacturing complexity, brand positioning, or customer preferences.

## Log transformed price verses rating

```{r}
ggplot_rp_tf = wine_rating |> 
  ggplot(aes(x = log(price), y = rating, color = year)) +
  geom_point()
ggplotly(ggplot_rp_tf)
```

Already examined the linear relationship scatter plot of the wine rating verses wine price, we noticed that it's hardly to be described as a linear relationship based on the distribution of the points. So, we performed logarithm transformation to the price variable and plotted a scatter plot again. The result is somehow more satisfied compared with previous result. The point are more clustered and formed a positive-linear liked relation. To perform more accurate linear model, we thought other variables might also get involved in this regression model. It is more likely to be a multiple expanded linear regression.

## Fitting linear model to rating, price, and year(residual plot)

```{r}
lm_rp = lm(rating ~ log(price) + year, data = wine_rating)

broom::tidy(lm_rp) |> 
  knitr::kable(digits = 3) |> 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), font_size = 12) |> 
  kableExtra::scroll_box(width = "100%", height = "300px")
```

With the thoughts in mind, we performed the MLR with wine rating, log transformed wine price, and corresponding year of the wine production. The result are shown in the above table. Things are getting a little bit tricky. We noticed that `p.value` of log transformed price is nearly zero, meaning there is strong linear relation between it and the dependent variable. But, most of the years have a `p.value` that is much greater than the significant level. More justification method are needed to help provide the evidence to judge the true relation between year and rating. So, we plotted a residual plot for this linear regression model.

```{r}
resid_rp = wine_rating |> 
  modelr::add_residuals(lm_rp) |> 
  ggplot(aes(x = log(price), y = resid)) + geom_point()
ggplotly(resid_rp)
```

From the plot, it's pretty obvious that the dataset needs more improvement for the model to fit better since all the point are not evenly spread acorss the axis. They are more condensed as the log transformed price increases. But, with such results, we cannot make absolute decision about the model we built is false one. To further test our model, we performed a simulation.

## Performing Bootstrap for above model for further test

```{r}
bootstrap_rp = wine_rating |> 
  modelr::bootstrap(n = 1000) |> 
  mutate(
    models = map(strap, \(df) lm(rating ~ log(price) + year, data = df)),
    results = map(models, broom::tidy)) |> 
  select(results) |> 
  unnest(results) |> 
  filter(term == "log(price)") |> 
  ggplot(aes(x = estimate)) + geom_density()
ggplotly(bootstrap_rp)
```

From the above bootstrap model, we obtained a perfect normal distribution of our frequencies of estimation. This helps proved that somehow our model is accurate for no extreme outliers or other strange data points occurred. Also, the distribution of density curve is surprisingly normal distributed, with no skewness and shoulders. This provide positive evidence for our linear regression model. Still, more tests are needed for the model to be exactly accurate. 